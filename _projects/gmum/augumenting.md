---
title: Augumenting SGD optimizers with low dimensional 2nd order information
leader: Jarek Duda
contact: jaroslaw.duda@uj.edu.pl
positions:
    - name: Student Researcher
lab_name: GMUM
created: 2023-10-22
location: Poland
---

SGD optimization is currently dominated by 1st order methods like Adam. Augumenting them with 2nd order information would suggest e.g. optimal step size. Such online parabola model can be maintained nearly for free by extracting linear trends of the gradient sequence (arXiv: 1907.07063), and is planned to be included for improving standard methods like Adam.

### General must-have requirements

The students needs to know basics of tensor flow or pytorch, preferred experience in mathematical analysis.
